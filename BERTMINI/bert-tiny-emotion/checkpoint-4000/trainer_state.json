{
  "best_metric": 0.34414759278297424,
  "best_model_checkpoint": "./bert-tiny-emotion/checkpoint-4000",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 2.708383083343506,
      "learning_rate": 4.99e-05,
      "loss": 1.7644,
      "step": 10
    },
    {
      "epoch": 0.02,
      "grad_norm": 2.88204026222229,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 1.7314,
      "step": 20
    },
    {
      "epoch": 0.03,
      "grad_norm": 2.23744535446167,
      "learning_rate": 4.97e-05,
      "loss": 1.7111,
      "step": 30
    },
    {
      "epoch": 0.04,
      "grad_norm": 3.0312910079956055,
      "learning_rate": 4.96e-05,
      "loss": 1.679,
      "step": 40
    },
    {
      "epoch": 0.05,
      "grad_norm": 3.5762417316436768,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 1.6759,
      "step": 50
    },
    {
      "epoch": 0.06,
      "grad_norm": 2.430635690689087,
      "learning_rate": 4.94e-05,
      "loss": 1.6552,
      "step": 60
    },
    {
      "epoch": 0.07,
      "grad_norm": 2.6336488723754883,
      "learning_rate": 4.93e-05,
      "loss": 1.6221,
      "step": 70
    },
    {
      "epoch": 0.08,
      "grad_norm": 2.1537511348724365,
      "learning_rate": 4.92e-05,
      "loss": 1.5988,
      "step": 80
    },
    {
      "epoch": 0.09,
      "grad_norm": 2.660006284713745,
      "learning_rate": 4.91e-05,
      "loss": 1.6077,
      "step": 90
    },
    {
      "epoch": 0.1,
      "grad_norm": 2.3159446716308594,
      "learning_rate": 4.9e-05,
      "loss": 1.6335,
      "step": 100
    },
    {
      "epoch": 0.11,
      "grad_norm": 3.795093059539795,
      "learning_rate": 4.89e-05,
      "loss": 1.6072,
      "step": 110
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.9214073419570923,
      "learning_rate": 4.88e-05,
      "loss": 1.5805,
      "step": 120
    },
    {
      "epoch": 0.13,
      "grad_norm": 1.5060158967971802,
      "learning_rate": 4.87e-05,
      "loss": 1.6173,
      "step": 130
    },
    {
      "epoch": 0.14,
      "grad_norm": 2.215106248855591,
      "learning_rate": 4.86e-05,
      "loss": 1.58,
      "step": 140
    },
    {
      "epoch": 0.15,
      "grad_norm": 2.3569118976593018,
      "learning_rate": 4.85e-05,
      "loss": 1.6175,
      "step": 150
    },
    {
      "epoch": 0.16,
      "grad_norm": 2.0220212936401367,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 1.5984,
      "step": 160
    },
    {
      "epoch": 0.17,
      "grad_norm": 2.344064474105835,
      "learning_rate": 4.83e-05,
      "loss": 1.5715,
      "step": 170
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.5285608768463135,
      "learning_rate": 4.82e-05,
      "loss": 1.5945,
      "step": 180
    },
    {
      "epoch": 0.19,
      "grad_norm": 3.1345860958099365,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 1.4742,
      "step": 190
    },
    {
      "epoch": 0.2,
      "grad_norm": 2.027193546295166,
      "learning_rate": 4.8e-05,
      "loss": 1.6198,
      "step": 200
    },
    {
      "epoch": 0.21,
      "grad_norm": 2.2507967948913574,
      "learning_rate": 4.79e-05,
      "loss": 1.5647,
      "step": 210
    },
    {
      "epoch": 0.22,
      "grad_norm": 2.6791579723358154,
      "learning_rate": 4.78e-05,
      "loss": 1.5563,
      "step": 220
    },
    {
      "epoch": 0.23,
      "grad_norm": 3.327056646347046,
      "learning_rate": 4.77e-05,
      "loss": 1.5326,
      "step": 230
    },
    {
      "epoch": 0.24,
      "grad_norm": 3.088818311691284,
      "learning_rate": 4.76e-05,
      "loss": 1.5815,
      "step": 240
    },
    {
      "epoch": 0.25,
      "grad_norm": 3.2855632305145264,
      "learning_rate": 4.75e-05,
      "loss": 1.5231,
      "step": 250
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.9063591957092285,
      "learning_rate": 4.74e-05,
      "loss": 1.5978,
      "step": 260
    },
    {
      "epoch": 0.27,
      "grad_norm": 2.034148931503296,
      "learning_rate": 4.73e-05,
      "loss": 1.5171,
      "step": 270
    },
    {
      "epoch": 0.28,
      "grad_norm": 2.3909544944763184,
      "learning_rate": 4.72e-05,
      "loss": 1.5544,
      "step": 280
    },
    {
      "epoch": 0.29,
      "grad_norm": 3.003913640975952,
      "learning_rate": 4.71e-05,
      "loss": 1.5197,
      "step": 290
    },
    {
      "epoch": 0.3,
      "grad_norm": 2.5439436435699463,
      "learning_rate": 4.7e-05,
      "loss": 1.5273,
      "step": 300
    },
    {
      "epoch": 0.31,
      "grad_norm": 2.734386920928955,
      "learning_rate": 4.69e-05,
      "loss": 1.5894,
      "step": 310
    },
    {
      "epoch": 0.32,
      "grad_norm": 2.3443500995635986,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 1.5537,
      "step": 320
    },
    {
      "epoch": 0.33,
      "grad_norm": 2.8961615562438965,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 1.4825,
      "step": 330
    },
    {
      "epoch": 0.34,
      "grad_norm": 2.5335960388183594,
      "learning_rate": 4.660000000000001e-05,
      "loss": 1.5111,
      "step": 340
    },
    {
      "epoch": 0.35,
      "grad_norm": 2.4616668224334717,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 1.4757,
      "step": 350
    },
    {
      "epoch": 0.36,
      "grad_norm": 2.033426523208618,
      "learning_rate": 4.64e-05,
      "loss": 1.5176,
      "step": 360
    },
    {
      "epoch": 0.37,
      "grad_norm": 2.903552293777466,
      "learning_rate": 4.630000000000001e-05,
      "loss": 1.4566,
      "step": 370
    },
    {
      "epoch": 0.38,
      "grad_norm": 3.4096016883850098,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 1.3838,
      "step": 380
    },
    {
      "epoch": 0.39,
      "grad_norm": 4.161189556121826,
      "learning_rate": 4.61e-05,
      "loss": 1.4117,
      "step": 390
    },
    {
      "epoch": 0.4,
      "grad_norm": 6.087362289428711,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.4374,
      "step": 400
    },
    {
      "epoch": 0.41,
      "grad_norm": 2.9960744380950928,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 1.4362,
      "step": 410
    },
    {
      "epoch": 0.42,
      "grad_norm": 3.4553604125976562,
      "learning_rate": 4.58e-05,
      "loss": 1.2767,
      "step": 420
    },
    {
      "epoch": 0.43,
      "grad_norm": 5.768896579742432,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 1.3939,
      "step": 430
    },
    {
      "epoch": 0.44,
      "grad_norm": 2.963930606842041,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 1.3024,
      "step": 440
    },
    {
      "epoch": 0.45,
      "grad_norm": 4.127689838409424,
      "learning_rate": 4.55e-05,
      "loss": 1.2913,
      "step": 450
    },
    {
      "epoch": 0.46,
      "grad_norm": 2.7683145999908447,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 1.3465,
      "step": 460
    },
    {
      "epoch": 0.47,
      "grad_norm": 3.25600266456604,
      "learning_rate": 4.53e-05,
      "loss": 1.2835,
      "step": 470
    },
    {
      "epoch": 0.48,
      "grad_norm": 3.5877745151519775,
      "learning_rate": 4.52e-05,
      "loss": 1.2835,
      "step": 480
    },
    {
      "epoch": 0.49,
      "grad_norm": 4.439813613891602,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 1.2911,
      "step": 490
    },
    {
      "epoch": 0.5,
      "grad_norm": 6.2463555335998535,
      "learning_rate": 4.5e-05,
      "loss": 1.2463,
      "step": 500
    },
    {
      "epoch": 0.51,
      "grad_norm": 3.622312068939209,
      "learning_rate": 4.49e-05,
      "loss": 1.2339,
      "step": 510
    },
    {
      "epoch": 0.52,
      "grad_norm": 4.309123992919922,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 1.2485,
      "step": 520
    },
    {
      "epoch": 0.53,
      "grad_norm": 4.883510589599609,
      "learning_rate": 4.47e-05,
      "loss": 1.2474,
      "step": 530
    },
    {
      "epoch": 0.54,
      "grad_norm": 3.3939223289489746,
      "learning_rate": 4.46e-05,
      "loss": 1.1895,
      "step": 540
    },
    {
      "epoch": 0.55,
      "grad_norm": 7.575652122497559,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 1.2543,
      "step": 550
    },
    {
      "epoch": 0.56,
      "grad_norm": 4.779197692871094,
      "learning_rate": 4.44e-05,
      "loss": 1.1921,
      "step": 560
    },
    {
      "epoch": 0.57,
      "grad_norm": 3.694565534591675,
      "learning_rate": 4.43e-05,
      "loss": 1.0807,
      "step": 570
    },
    {
      "epoch": 0.58,
      "grad_norm": 3.4429402351379395,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 1.2091,
      "step": 580
    },
    {
      "epoch": 0.59,
      "grad_norm": 4.8655924797058105,
      "learning_rate": 4.41e-05,
      "loss": 1.1501,
      "step": 590
    },
    {
      "epoch": 0.6,
      "grad_norm": 4.362054347991943,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.0439,
      "step": 600
    },
    {
      "epoch": 0.61,
      "grad_norm": 3.550171136856079,
      "learning_rate": 4.39e-05,
      "loss": 1.1477,
      "step": 610
    },
    {
      "epoch": 0.62,
      "grad_norm": 3.5766351222991943,
      "learning_rate": 4.38e-05,
      "loss": 1.1612,
      "step": 620
    },
    {
      "epoch": 0.63,
      "grad_norm": 4.577878952026367,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 1.1137,
      "step": 630
    },
    {
      "epoch": 0.64,
      "grad_norm": 7.744543075561523,
      "learning_rate": 4.36e-05,
      "loss": 1.146,
      "step": 640
    },
    {
      "epoch": 0.65,
      "grad_norm": 4.435516834259033,
      "learning_rate": 4.35e-05,
      "loss": 1.2147,
      "step": 650
    },
    {
      "epoch": 0.66,
      "grad_norm": 4.473118782043457,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 1.1234,
      "step": 660
    },
    {
      "epoch": 0.67,
      "grad_norm": 5.256957530975342,
      "learning_rate": 4.33e-05,
      "loss": 1.0214,
      "step": 670
    },
    {
      "epoch": 0.68,
      "grad_norm": 5.194881916046143,
      "learning_rate": 4.32e-05,
      "loss": 1.0724,
      "step": 680
    },
    {
      "epoch": 0.69,
      "grad_norm": 4.197247505187988,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.9539,
      "step": 690
    },
    {
      "epoch": 0.7,
      "grad_norm": 3.9927940368652344,
      "learning_rate": 4.3e-05,
      "loss": 1.0862,
      "step": 700
    },
    {
      "epoch": 0.71,
      "grad_norm": 7.576542854309082,
      "learning_rate": 4.29e-05,
      "loss": 1.0111,
      "step": 710
    },
    {
      "epoch": 0.72,
      "grad_norm": 4.544802665710449,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 1.0592,
      "step": 720
    },
    {
      "epoch": 0.73,
      "grad_norm": 15.61481761932373,
      "learning_rate": 4.27e-05,
      "loss": 1.0395,
      "step": 730
    },
    {
      "epoch": 0.74,
      "grad_norm": 3.9797606468200684,
      "learning_rate": 4.26e-05,
      "loss": 1.0018,
      "step": 740
    },
    {
      "epoch": 0.75,
      "grad_norm": 3.5250604152679443,
      "learning_rate": 4.25e-05,
      "loss": 1.0285,
      "step": 750
    },
    {
      "epoch": 0.76,
      "grad_norm": 5.857987880706787,
      "learning_rate": 4.24e-05,
      "loss": 1.013,
      "step": 760
    },
    {
      "epoch": 0.77,
      "grad_norm": 3.6682093143463135,
      "learning_rate": 4.23e-05,
      "loss": 1.0268,
      "step": 770
    },
    {
      "epoch": 0.78,
      "grad_norm": 4.832491874694824,
      "learning_rate": 4.22e-05,
      "loss": 0.988,
      "step": 780
    },
    {
      "epoch": 0.79,
      "grad_norm": 8.9746675491333,
      "learning_rate": 4.21e-05,
      "loss": 1.1084,
      "step": 790
    },
    {
      "epoch": 0.8,
      "grad_norm": 6.080558776855469,
      "learning_rate": 4.2e-05,
      "loss": 0.9652,
      "step": 800
    },
    {
      "epoch": 0.81,
      "grad_norm": 5.321732997894287,
      "learning_rate": 4.19e-05,
      "loss": 1.0268,
      "step": 810
    },
    {
      "epoch": 0.82,
      "grad_norm": 6.131740570068359,
      "learning_rate": 4.18e-05,
      "loss": 0.815,
      "step": 820
    },
    {
      "epoch": 0.83,
      "grad_norm": 4.849005222320557,
      "learning_rate": 4.17e-05,
      "loss": 0.9905,
      "step": 830
    },
    {
      "epoch": 0.84,
      "grad_norm": 5.057007789611816,
      "learning_rate": 4.16e-05,
      "loss": 0.8401,
      "step": 840
    },
    {
      "epoch": 0.85,
      "grad_norm": 6.809078216552734,
      "learning_rate": 4.15e-05,
      "loss": 0.8966,
      "step": 850
    },
    {
      "epoch": 0.86,
      "grad_norm": 7.691864490509033,
      "learning_rate": 4.14e-05,
      "loss": 1.0182,
      "step": 860
    },
    {
      "epoch": 0.87,
      "grad_norm": 6.750192642211914,
      "learning_rate": 4.13e-05,
      "loss": 0.951,
      "step": 870
    },
    {
      "epoch": 0.88,
      "grad_norm": 6.922979831695557,
      "learning_rate": 4.12e-05,
      "loss": 0.9499,
      "step": 880
    },
    {
      "epoch": 0.89,
      "grad_norm": 7.39340877532959,
      "learning_rate": 4.11e-05,
      "loss": 0.9101,
      "step": 890
    },
    {
      "epoch": 0.9,
      "grad_norm": 4.3804473876953125,
      "learning_rate": 4.1e-05,
      "loss": 0.9436,
      "step": 900
    },
    {
      "epoch": 0.91,
      "grad_norm": 9.779831886291504,
      "learning_rate": 4.09e-05,
      "loss": 0.9485,
      "step": 910
    },
    {
      "epoch": 0.92,
      "grad_norm": 5.217221260070801,
      "learning_rate": 4.08e-05,
      "loss": 0.9907,
      "step": 920
    },
    {
      "epoch": 0.93,
      "grad_norm": 7.0860748291015625,
      "learning_rate": 4.07e-05,
      "loss": 0.8856,
      "step": 930
    },
    {
      "epoch": 0.94,
      "grad_norm": 10.35302448272705,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.9458,
      "step": 940
    },
    {
      "epoch": 0.95,
      "grad_norm": 12.365478515625,
      "learning_rate": 4.05e-05,
      "loss": 0.9008,
      "step": 950
    },
    {
      "epoch": 0.96,
      "grad_norm": 6.281249523162842,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.7635,
      "step": 960
    },
    {
      "epoch": 0.97,
      "grad_norm": 7.596977710723877,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 0.9125,
      "step": 970
    },
    {
      "epoch": 0.98,
      "grad_norm": 6.417993545532227,
      "learning_rate": 4.02e-05,
      "loss": 0.9255,
      "step": 980
    },
    {
      "epoch": 0.99,
      "grad_norm": 4.74734354019165,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.8127,
      "step": 990
    },
    {
      "epoch": 1.0,
      "grad_norm": 6.768809795379639,
      "learning_rate": 4e-05,
      "loss": 0.9066,
      "step": 1000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7775,
      "eval_loss": 0.7820656895637512,
      "eval_runtime": 4.4882,
      "eval_samples_per_second": 445.61,
      "eval_steps_per_second": 7.13,
      "step": 1000
    },
    {
      "epoch": 1.01,
      "grad_norm": 4.959724426269531,
      "learning_rate": 3.99e-05,
      "loss": 0.7737,
      "step": 1010
    },
    {
      "epoch": 1.02,
      "grad_norm": 3.9881811141967773,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.8276,
      "step": 1020
    },
    {
      "epoch": 1.03,
      "grad_norm": 5.674715995788574,
      "learning_rate": 3.97e-05,
      "loss": 0.7586,
      "step": 1030
    },
    {
      "epoch": 1.04,
      "grad_norm": 7.160725116729736,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.8028,
      "step": 1040
    },
    {
      "epoch": 1.05,
      "grad_norm": 5.843245029449463,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.8014,
      "step": 1050
    },
    {
      "epoch": 1.06,
      "grad_norm": 10.402310371398926,
      "learning_rate": 3.94e-05,
      "loss": 0.7878,
      "step": 1060
    },
    {
      "epoch": 1.07,
      "grad_norm": 4.757682800292969,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 0.7755,
      "step": 1070
    },
    {
      "epoch": 1.08,
      "grad_norm": 9.914316177368164,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.8015,
      "step": 1080
    },
    {
      "epoch": 1.09,
      "grad_norm": 2.9910972118377686,
      "learning_rate": 3.91e-05,
      "loss": 0.6922,
      "step": 1090
    },
    {
      "epoch": 1.1,
      "grad_norm": 8.767078399658203,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.8077,
      "step": 1100
    },
    {
      "epoch": 1.11,
      "grad_norm": 2.802199602127075,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 0.7164,
      "step": 1110
    },
    {
      "epoch": 1.12,
      "grad_norm": 5.022088527679443,
      "learning_rate": 3.88e-05,
      "loss": 0.7238,
      "step": 1120
    },
    {
      "epoch": 1.13,
      "grad_norm": 8.39925479888916,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.7578,
      "step": 1130
    },
    {
      "epoch": 1.14,
      "grad_norm": 10.903443336486816,
      "learning_rate": 3.86e-05,
      "loss": 0.7385,
      "step": 1140
    },
    {
      "epoch": 1.15,
      "grad_norm": 4.9546217918396,
      "learning_rate": 3.85e-05,
      "loss": 0.6368,
      "step": 1150
    },
    {
      "epoch": 1.16,
      "grad_norm": 3.5127720832824707,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.7318,
      "step": 1160
    },
    {
      "epoch": 1.17,
      "grad_norm": 11.738779067993164,
      "learning_rate": 3.83e-05,
      "loss": 0.69,
      "step": 1170
    },
    {
      "epoch": 1.18,
      "grad_norm": 9.865364074707031,
      "learning_rate": 3.82e-05,
      "loss": 0.7068,
      "step": 1180
    },
    {
      "epoch": 1.19,
      "grad_norm": 8.910664558410645,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.7605,
      "step": 1190
    },
    {
      "epoch": 1.2,
      "grad_norm": 6.513505458831787,
      "learning_rate": 3.8e-05,
      "loss": 0.7024,
      "step": 1200
    },
    {
      "epoch": 1.21,
      "grad_norm": 9.229788780212402,
      "learning_rate": 3.79e-05,
      "loss": 0.7522,
      "step": 1210
    },
    {
      "epoch": 1.22,
      "grad_norm": 13.540505409240723,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.7003,
      "step": 1220
    },
    {
      "epoch": 1.23,
      "grad_norm": 10.398788452148438,
      "learning_rate": 3.77e-05,
      "loss": 0.7416,
      "step": 1230
    },
    {
      "epoch": 1.24,
      "grad_norm": 6.4492645263671875,
      "learning_rate": 3.76e-05,
      "loss": 0.7248,
      "step": 1240
    },
    {
      "epoch": 1.25,
      "grad_norm": 7.148509502410889,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.6438,
      "step": 1250
    },
    {
      "epoch": 1.26,
      "grad_norm": 8.946046829223633,
      "learning_rate": 3.74e-05,
      "loss": 0.6954,
      "step": 1260
    },
    {
      "epoch": 1.27,
      "grad_norm": 3.107842206954956,
      "learning_rate": 3.73e-05,
      "loss": 0.6517,
      "step": 1270
    },
    {
      "epoch": 1.28,
      "grad_norm": 5.614877700805664,
      "learning_rate": 3.72e-05,
      "loss": 0.7211,
      "step": 1280
    },
    {
      "epoch": 1.29,
      "grad_norm": 13.848888397216797,
      "learning_rate": 3.71e-05,
      "loss": 0.6394,
      "step": 1290
    },
    {
      "epoch": 1.3,
      "grad_norm": 4.776382923126221,
      "learning_rate": 3.7e-05,
      "loss": 0.7578,
      "step": 1300
    },
    {
      "epoch": 1.31,
      "grad_norm": 5.5748677253723145,
      "learning_rate": 3.69e-05,
      "loss": 0.6904,
      "step": 1310
    },
    {
      "epoch": 1.32,
      "grad_norm": 12.828302383422852,
      "learning_rate": 3.68e-05,
      "loss": 0.7867,
      "step": 1320
    },
    {
      "epoch": 1.33,
      "grad_norm": 4.659051418304443,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.6945,
      "step": 1330
    },
    {
      "epoch": 1.34,
      "grad_norm": 2.4258954524993896,
      "learning_rate": 3.66e-05,
      "loss": 0.675,
      "step": 1340
    },
    {
      "epoch": 1.35,
      "grad_norm": 5.265585899353027,
      "learning_rate": 3.65e-05,
      "loss": 0.6504,
      "step": 1350
    },
    {
      "epoch": 1.36,
      "grad_norm": 6.465514183044434,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.6915,
      "step": 1360
    },
    {
      "epoch": 1.37,
      "grad_norm": 11.924847602844238,
      "learning_rate": 3.63e-05,
      "loss": 0.697,
      "step": 1370
    },
    {
      "epoch": 1.38,
      "grad_norm": 6.665163516998291,
      "learning_rate": 3.62e-05,
      "loss": 0.5653,
      "step": 1380
    },
    {
      "epoch": 1.39,
      "grad_norm": 9.53304672241211,
      "learning_rate": 3.61e-05,
      "loss": 0.6036,
      "step": 1390
    },
    {
      "epoch": 1.4,
      "grad_norm": 10.044482231140137,
      "learning_rate": 3.6e-05,
      "loss": 0.7139,
      "step": 1400
    },
    {
      "epoch": 1.41,
      "grad_norm": 4.774266719818115,
      "learning_rate": 3.59e-05,
      "loss": 0.6236,
      "step": 1410
    },
    {
      "epoch": 1.42,
      "grad_norm": 8.72064208984375,
      "learning_rate": 3.58e-05,
      "loss": 0.8061,
      "step": 1420
    },
    {
      "epoch": 1.43,
      "grad_norm": 5.429601192474365,
      "learning_rate": 3.57e-05,
      "loss": 0.6702,
      "step": 1430
    },
    {
      "epoch": 1.44,
      "grad_norm": 7.627901077270508,
      "learning_rate": 3.56e-05,
      "loss": 0.6496,
      "step": 1440
    },
    {
      "epoch": 1.45,
      "grad_norm": 5.937588214874268,
      "learning_rate": 3.55e-05,
      "loss": 0.6228,
      "step": 1450
    },
    {
      "epoch": 1.46,
      "grad_norm": 8.493903160095215,
      "learning_rate": 3.54e-05,
      "loss": 0.6445,
      "step": 1460
    },
    {
      "epoch": 1.47,
      "grad_norm": 6.55935001373291,
      "learning_rate": 3.53e-05,
      "loss": 0.662,
      "step": 1470
    },
    {
      "epoch": 1.48,
      "grad_norm": 12.531036376953125,
      "learning_rate": 3.52e-05,
      "loss": 0.6694,
      "step": 1480
    },
    {
      "epoch": 1.49,
      "grad_norm": 7.88460636138916,
      "learning_rate": 3.51e-05,
      "loss": 0.6353,
      "step": 1490
    },
    {
      "epoch": 1.5,
      "grad_norm": 6.1101298332214355,
      "learning_rate": 3.5e-05,
      "loss": 0.5354,
      "step": 1500
    },
    {
      "epoch": 1.51,
      "grad_norm": 4.884852409362793,
      "learning_rate": 3.49e-05,
      "loss": 0.5752,
      "step": 1510
    },
    {
      "epoch": 1.52,
      "grad_norm": 15.334498405456543,
      "learning_rate": 3.48e-05,
      "loss": 0.6271,
      "step": 1520
    },
    {
      "epoch": 1.53,
      "grad_norm": 14.380607604980469,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.6015,
      "step": 1530
    },
    {
      "epoch": 1.54,
      "grad_norm": 5.437220573425293,
      "learning_rate": 3.46e-05,
      "loss": 0.6258,
      "step": 1540
    },
    {
      "epoch": 1.55,
      "grad_norm": 5.373530864715576,
      "learning_rate": 3.45e-05,
      "loss": 0.5554,
      "step": 1550
    },
    {
      "epoch": 1.56,
      "grad_norm": 5.44745397567749,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.6563,
      "step": 1560
    },
    {
      "epoch": 1.57,
      "grad_norm": 4.934607982635498,
      "learning_rate": 3.430000000000001e-05,
      "loss": 0.5986,
      "step": 1570
    },
    {
      "epoch": 1.58,
      "grad_norm": 7.066926956176758,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.5417,
      "step": 1580
    },
    {
      "epoch": 1.59,
      "grad_norm": 10.41871166229248,
      "learning_rate": 3.41e-05,
      "loss": 0.4969,
      "step": 1590
    },
    {
      "epoch": 1.6,
      "grad_norm": 3.5513193607330322,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.5519,
      "step": 1600
    },
    {
      "epoch": 1.61,
      "grad_norm": 12.45307445526123,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.6517,
      "step": 1610
    },
    {
      "epoch": 1.62,
      "grad_norm": 3.4787657260894775,
      "learning_rate": 3.38e-05,
      "loss": 0.5781,
      "step": 1620
    },
    {
      "epoch": 1.63,
      "grad_norm": 7.7484354972839355,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.6572,
      "step": 1630
    },
    {
      "epoch": 1.64,
      "grad_norm": 10.829270362854004,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.6102,
      "step": 1640
    },
    {
      "epoch": 1.65,
      "grad_norm": 12.250707626342773,
      "learning_rate": 3.35e-05,
      "loss": 0.5731,
      "step": 1650
    },
    {
      "epoch": 1.66,
      "grad_norm": 8.348196983337402,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.5915,
      "step": 1660
    },
    {
      "epoch": 1.67,
      "grad_norm": 4.796567440032959,
      "learning_rate": 3.33e-05,
      "loss": 0.5667,
      "step": 1670
    },
    {
      "epoch": 1.68,
      "grad_norm": 5.035577297210693,
      "learning_rate": 3.32e-05,
      "loss": 0.5563,
      "step": 1680
    },
    {
      "epoch": 1.69,
      "grad_norm": 5.452916145324707,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.6146,
      "step": 1690
    },
    {
      "epoch": 1.7,
      "grad_norm": 6.94737434387207,
      "learning_rate": 3.3e-05,
      "loss": 0.5675,
      "step": 1700
    },
    {
      "epoch": 1.71,
      "grad_norm": 7.366430759429932,
      "learning_rate": 3.29e-05,
      "loss": 0.6392,
      "step": 1710
    },
    {
      "epoch": 1.72,
      "grad_norm": 5.085954666137695,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.5622,
      "step": 1720
    },
    {
      "epoch": 1.73,
      "grad_norm": 4.794347286224365,
      "learning_rate": 3.27e-05,
      "loss": 0.6196,
      "step": 1730
    },
    {
      "epoch": 1.74,
      "grad_norm": 4.317324161529541,
      "learning_rate": 3.26e-05,
      "loss": 0.5239,
      "step": 1740
    },
    {
      "epoch": 1.75,
      "grad_norm": 11.641158103942871,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.684,
      "step": 1750
    },
    {
      "epoch": 1.76,
      "grad_norm": 4.770768165588379,
      "learning_rate": 3.24e-05,
      "loss": 0.5309,
      "step": 1760
    },
    {
      "epoch": 1.77,
      "grad_norm": 8.185792922973633,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 0.4603,
      "step": 1770
    },
    {
      "epoch": 1.78,
      "grad_norm": 7.316680431365967,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.5056,
      "step": 1780
    },
    {
      "epoch": 1.79,
      "grad_norm": 12.207955360412598,
      "learning_rate": 3.21e-05,
      "loss": 0.5442,
      "step": 1790
    },
    {
      "epoch": 1.8,
      "grad_norm": 8.152262687683105,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.5145,
      "step": 1800
    },
    {
      "epoch": 1.81,
      "grad_norm": 10.879014015197754,
      "learning_rate": 3.19e-05,
      "loss": 0.4732,
      "step": 1810
    },
    {
      "epoch": 1.82,
      "grad_norm": 10.57034683227539,
      "learning_rate": 3.18e-05,
      "loss": 0.6697,
      "step": 1820
    },
    {
      "epoch": 1.83,
      "grad_norm": 7.482059478759766,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.5186,
      "step": 1830
    },
    {
      "epoch": 1.84,
      "grad_norm": 4.487321853637695,
      "learning_rate": 3.16e-05,
      "loss": 0.6468,
      "step": 1840
    },
    {
      "epoch": 1.85,
      "grad_norm": 11.275839805603027,
      "learning_rate": 3.15e-05,
      "loss": 0.5577,
      "step": 1850
    },
    {
      "epoch": 1.86,
      "grad_norm": 4.9824299812316895,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.5754,
      "step": 1860
    },
    {
      "epoch": 1.87,
      "grad_norm": 14.232675552368164,
      "learning_rate": 3.13e-05,
      "loss": 0.5304,
      "step": 1870
    },
    {
      "epoch": 1.88,
      "grad_norm": 5.352158546447754,
      "learning_rate": 3.12e-05,
      "loss": 0.5067,
      "step": 1880
    },
    {
      "epoch": 1.89,
      "grad_norm": 11.13895034790039,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.5174,
      "step": 1890
    },
    {
      "epoch": 1.9,
      "grad_norm": 11.316357612609863,
      "learning_rate": 3.1e-05,
      "loss": 0.4562,
      "step": 1900
    },
    {
      "epoch": 1.91,
      "grad_norm": 7.7125983238220215,
      "learning_rate": 3.09e-05,
      "loss": 0.5347,
      "step": 1910
    },
    {
      "epoch": 1.92,
      "grad_norm": 7.363845348358154,
      "learning_rate": 3.08e-05,
      "loss": 0.5691,
      "step": 1920
    },
    {
      "epoch": 1.93,
      "grad_norm": 13.527926445007324,
      "learning_rate": 3.07e-05,
      "loss": 0.4689,
      "step": 1930
    },
    {
      "epoch": 1.94,
      "grad_norm": 14.742980003356934,
      "learning_rate": 3.06e-05,
      "loss": 0.4967,
      "step": 1940
    },
    {
      "epoch": 1.95,
      "grad_norm": 12.793669700622559,
      "learning_rate": 3.05e-05,
      "loss": 0.5126,
      "step": 1950
    },
    {
      "epoch": 1.96,
      "grad_norm": 5.888334274291992,
      "learning_rate": 3.04e-05,
      "loss": 0.508,
      "step": 1960
    },
    {
      "epoch": 1.97,
      "grad_norm": 8.921607971191406,
      "learning_rate": 3.03e-05,
      "loss": 0.5122,
      "step": 1970
    },
    {
      "epoch": 1.98,
      "grad_norm": 8.236230850219727,
      "learning_rate": 3.02e-05,
      "loss": 0.6403,
      "step": 1980
    },
    {
      "epoch": 1.99,
      "grad_norm": 10.78759479522705,
      "learning_rate": 3.01e-05,
      "loss": 0.5336,
      "step": 1990
    },
    {
      "epoch": 2.0,
      "grad_norm": 9.880850791931152,
      "learning_rate": 3e-05,
      "loss": 0.5619,
      "step": 2000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8685,
      "eval_loss": 0.4658096134662628,
      "eval_runtime": 4.2487,
      "eval_samples_per_second": 470.729,
      "eval_steps_per_second": 7.532,
      "step": 2000
    },
    {
      "epoch": 2.01,
      "grad_norm": 11.767748832702637,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.4996,
      "step": 2010
    },
    {
      "epoch": 2.02,
      "grad_norm": 7.012530326843262,
      "learning_rate": 2.98e-05,
      "loss": 0.5225,
      "step": 2020
    },
    {
      "epoch": 2.03,
      "grad_norm": 5.310185432434082,
      "learning_rate": 2.97e-05,
      "loss": 0.4615,
      "step": 2030
    },
    {
      "epoch": 2.04,
      "grad_norm": 6.274841785430908,
      "learning_rate": 2.96e-05,
      "loss": 0.5476,
      "step": 2040
    },
    {
      "epoch": 2.05,
      "grad_norm": 6.81354284286499,
      "learning_rate": 2.95e-05,
      "loss": 0.4821,
      "step": 2050
    },
    {
      "epoch": 2.06,
      "grad_norm": 4.722141742706299,
      "learning_rate": 2.94e-05,
      "loss": 0.5229,
      "step": 2060
    },
    {
      "epoch": 2.07,
      "grad_norm": 9.556273460388184,
      "learning_rate": 2.93e-05,
      "loss": 0.5764,
      "step": 2070
    },
    {
      "epoch": 2.08,
      "grad_norm": 1.5948461294174194,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.4623,
      "step": 2080
    },
    {
      "epoch": 2.09,
      "grad_norm": 6.494404315948486,
      "learning_rate": 2.91e-05,
      "loss": 0.5,
      "step": 2090
    },
    {
      "epoch": 2.1,
      "grad_norm": 7.00092887878418,
      "learning_rate": 2.9e-05,
      "loss": 0.4195,
      "step": 2100
    },
    {
      "epoch": 2.11,
      "grad_norm": 2.533543348312378,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 0.4748,
      "step": 2110
    },
    {
      "epoch": 2.12,
      "grad_norm": 5.6596198081970215,
      "learning_rate": 2.88e-05,
      "loss": 0.547,
      "step": 2120
    },
    {
      "epoch": 2.13,
      "grad_norm": 4.015748500823975,
      "learning_rate": 2.87e-05,
      "loss": 0.4662,
      "step": 2130
    },
    {
      "epoch": 2.14,
      "grad_norm": 6.699817657470703,
      "learning_rate": 2.86e-05,
      "loss": 0.3253,
      "step": 2140
    },
    {
      "epoch": 2.15,
      "grad_norm": 3.450413942337036,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.4653,
      "step": 2150
    },
    {
      "epoch": 2.16,
      "grad_norm": 4.628045082092285,
      "learning_rate": 2.84e-05,
      "loss": 0.4443,
      "step": 2160
    },
    {
      "epoch": 2.17,
      "grad_norm": 10.286616325378418,
      "learning_rate": 2.83e-05,
      "loss": 0.4635,
      "step": 2170
    },
    {
      "epoch": 2.18,
      "grad_norm": 11.7067232131958,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.4654,
      "step": 2180
    },
    {
      "epoch": 2.19,
      "grad_norm": 14.746790885925293,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.4128,
      "step": 2190
    },
    {
      "epoch": 2.2,
      "grad_norm": 2.8455705642700195,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.4293,
      "step": 2200
    },
    {
      "epoch": 2.21,
      "grad_norm": 4.7135725021362305,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.4894,
      "step": 2210
    },
    {
      "epoch": 2.22,
      "grad_norm": 7.025715351104736,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.364,
      "step": 2220
    },
    {
      "epoch": 2.23,
      "grad_norm": 7.6521124839782715,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.4297,
      "step": 2230
    },
    {
      "epoch": 2.24,
      "grad_norm": 9.690311431884766,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.4741,
      "step": 2240
    },
    {
      "epoch": 2.25,
      "grad_norm": 7.14651346206665,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.4142,
      "step": 2250
    },
    {
      "epoch": 2.26,
      "grad_norm": 12.068587303161621,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.4526,
      "step": 2260
    },
    {
      "epoch": 2.27,
      "grad_norm": 15.578619956970215,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 0.4211,
      "step": 2270
    },
    {
      "epoch": 2.28,
      "grad_norm": 12.099576950073242,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.5155,
      "step": 2280
    },
    {
      "epoch": 2.29,
      "grad_norm": 2.4907760620117188,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.413,
      "step": 2290
    },
    {
      "epoch": 2.3,
      "grad_norm": 9.601951599121094,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.4374,
      "step": 2300
    },
    {
      "epoch": 2.31,
      "grad_norm": 12.736842155456543,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 0.3758,
      "step": 2310
    },
    {
      "epoch": 2.32,
      "grad_norm": 16.507596969604492,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.4694,
      "step": 2320
    },
    {
      "epoch": 2.33,
      "grad_norm": 11.489921569824219,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.4084,
      "step": 2330
    },
    {
      "epoch": 2.34,
      "grad_norm": 11.269256591796875,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.3939,
      "step": 2340
    },
    {
      "epoch": 2.35,
      "grad_norm": 13.783419609069824,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.4388,
      "step": 2350
    },
    {
      "epoch": 2.36,
      "grad_norm": 4.557296276092529,
      "learning_rate": 2.64e-05,
      "loss": 0.3521,
      "step": 2360
    },
    {
      "epoch": 2.37,
      "grad_norm": 18.46160316467285,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 0.4236,
      "step": 2370
    },
    {
      "epoch": 2.38,
      "grad_norm": 25.31478500366211,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.4126,
      "step": 2380
    },
    {
      "epoch": 2.39,
      "grad_norm": 7.131563663482666,
      "learning_rate": 2.61e-05,
      "loss": 0.4203,
      "step": 2390
    },
    {
      "epoch": 2.4,
      "grad_norm": 10.894152641296387,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.4541,
      "step": 2400
    },
    {
      "epoch": 2.41,
      "grad_norm": 9.818016052246094,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.4927,
      "step": 2410
    },
    {
      "epoch": 2.42,
      "grad_norm": 5.2622456550598145,
      "learning_rate": 2.58e-05,
      "loss": 0.498,
      "step": 2420
    },
    {
      "epoch": 2.43,
      "grad_norm": 8.463570594787598,
      "learning_rate": 2.57e-05,
      "loss": 0.3826,
      "step": 2430
    },
    {
      "epoch": 2.44,
      "grad_norm": 7.7392659187316895,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.466,
      "step": 2440
    },
    {
      "epoch": 2.45,
      "grad_norm": 19.675647735595703,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.3612,
      "step": 2450
    },
    {
      "epoch": 2.46,
      "grad_norm": 6.242410182952881,
      "learning_rate": 2.54e-05,
      "loss": 0.4625,
      "step": 2460
    },
    {
      "epoch": 2.47,
      "grad_norm": 10.633663177490234,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 0.4463,
      "step": 2470
    },
    {
      "epoch": 2.48,
      "grad_norm": 6.111350059509277,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.3365,
      "step": 2480
    },
    {
      "epoch": 2.49,
      "grad_norm": 8.208091735839844,
      "learning_rate": 2.51e-05,
      "loss": 0.4021,
      "step": 2490
    },
    {
      "epoch": 2.5,
      "grad_norm": 7.664950847625732,
      "learning_rate": 2.5e-05,
      "loss": 0.3568,
      "step": 2500
    },
    {
      "epoch": 2.51,
      "grad_norm": 5.958224296569824,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 0.4593,
      "step": 2510
    },
    {
      "epoch": 2.52,
      "grad_norm": 12.146411895751953,
      "learning_rate": 2.48e-05,
      "loss": 0.5247,
      "step": 2520
    },
    {
      "epoch": 2.53,
      "grad_norm": 10.305088996887207,
      "learning_rate": 2.47e-05,
      "loss": 0.3637,
      "step": 2530
    },
    {
      "epoch": 2.54,
      "grad_norm": 15.227987289428711,
      "learning_rate": 2.46e-05,
      "loss": 0.4085,
      "step": 2540
    },
    {
      "epoch": 2.55,
      "grad_norm": 5.208439350128174,
      "learning_rate": 2.45e-05,
      "loss": 0.3027,
      "step": 2550
    },
    {
      "epoch": 2.56,
      "grad_norm": 6.398350715637207,
      "learning_rate": 2.44e-05,
      "loss": 0.4071,
      "step": 2560
    },
    {
      "epoch": 2.57,
      "grad_norm": 5.870028018951416,
      "learning_rate": 2.43e-05,
      "loss": 0.5079,
      "step": 2570
    },
    {
      "epoch": 2.58,
      "grad_norm": 15.22867202758789,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.2884,
      "step": 2580
    },
    {
      "epoch": 2.59,
      "grad_norm": 8.720536231994629,
      "learning_rate": 2.41e-05,
      "loss": 0.344,
      "step": 2590
    },
    {
      "epoch": 2.6,
      "grad_norm": 3.9794299602508545,
      "learning_rate": 2.4e-05,
      "loss": 0.4021,
      "step": 2600
    },
    {
      "epoch": 2.61,
      "grad_norm": 9.363140106201172,
      "learning_rate": 2.39e-05,
      "loss": 0.4492,
      "step": 2610
    },
    {
      "epoch": 2.62,
      "grad_norm": 5.558279514312744,
      "learning_rate": 2.38e-05,
      "loss": 0.5133,
      "step": 2620
    },
    {
      "epoch": 2.63,
      "grad_norm": 15.874471664428711,
      "learning_rate": 2.37e-05,
      "loss": 0.387,
      "step": 2630
    },
    {
      "epoch": 2.64,
      "grad_norm": 6.8899006843566895,
      "learning_rate": 2.36e-05,
      "loss": 0.4064,
      "step": 2640
    },
    {
      "epoch": 2.65,
      "grad_norm": 12.082404136657715,
      "learning_rate": 2.35e-05,
      "loss": 0.4307,
      "step": 2650
    },
    {
      "epoch": 2.66,
      "grad_norm": 17.473312377929688,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.4886,
      "step": 2660
    },
    {
      "epoch": 2.67,
      "grad_norm": 9.565390586853027,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 0.396,
      "step": 2670
    },
    {
      "epoch": 2.68,
      "grad_norm": 9.438864707946777,
      "learning_rate": 2.32e-05,
      "loss": 0.4213,
      "step": 2680
    },
    {
      "epoch": 2.69,
      "grad_norm": 14.314936637878418,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.5026,
      "step": 2690
    },
    {
      "epoch": 2.7,
      "grad_norm": 15.555173873901367,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.446,
      "step": 2700
    },
    {
      "epoch": 2.71,
      "grad_norm": 2.395853281021118,
      "learning_rate": 2.29e-05,
      "loss": 0.3546,
      "step": 2710
    },
    {
      "epoch": 2.72,
      "grad_norm": 5.888957977294922,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.4219,
      "step": 2720
    },
    {
      "epoch": 2.73,
      "grad_norm": 10.079520225524902,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.3215,
      "step": 2730
    },
    {
      "epoch": 2.74,
      "grad_norm": 3.990307331085205,
      "learning_rate": 2.26e-05,
      "loss": 0.4416,
      "step": 2740
    },
    {
      "epoch": 2.75,
      "grad_norm": 9.252400398254395,
      "learning_rate": 2.25e-05,
      "loss": 0.3084,
      "step": 2750
    },
    {
      "epoch": 2.76,
      "grad_norm": 8.597393989562988,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.3119,
      "step": 2760
    },
    {
      "epoch": 2.77,
      "grad_norm": 3.2237942218780518,
      "learning_rate": 2.23e-05,
      "loss": 0.3622,
      "step": 2770
    },
    {
      "epoch": 2.78,
      "grad_norm": 7.076282501220703,
      "learning_rate": 2.22e-05,
      "loss": 0.398,
      "step": 2780
    },
    {
      "epoch": 2.79,
      "grad_norm": 9.200651168823242,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.4257,
      "step": 2790
    },
    {
      "epoch": 2.8,
      "grad_norm": 11.615848541259766,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.5467,
      "step": 2800
    },
    {
      "epoch": 2.81,
      "grad_norm": 6.0428266525268555,
      "learning_rate": 2.19e-05,
      "loss": 0.3496,
      "step": 2810
    },
    {
      "epoch": 2.82,
      "grad_norm": 10.781597137451172,
      "learning_rate": 2.18e-05,
      "loss": 0.378,
      "step": 2820
    },
    {
      "epoch": 2.83,
      "grad_norm": 9.013348579406738,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.4833,
      "step": 2830
    },
    {
      "epoch": 2.84,
      "grad_norm": 10.514938354492188,
      "learning_rate": 2.16e-05,
      "loss": 0.3309,
      "step": 2840
    },
    {
      "epoch": 2.85,
      "grad_norm": 3.580094814300537,
      "learning_rate": 2.15e-05,
      "loss": 0.4881,
      "step": 2850
    },
    {
      "epoch": 2.86,
      "grad_norm": 5.35211706161499,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.4834,
      "step": 2860
    },
    {
      "epoch": 2.87,
      "grad_norm": 10.774834632873535,
      "learning_rate": 2.13e-05,
      "loss": 0.3817,
      "step": 2870
    },
    {
      "epoch": 2.88,
      "grad_norm": 13.497836112976074,
      "learning_rate": 2.12e-05,
      "loss": 0.4995,
      "step": 2880
    },
    {
      "epoch": 2.89,
      "grad_norm": 4.702185153961182,
      "learning_rate": 2.11e-05,
      "loss": 0.4192,
      "step": 2890
    },
    {
      "epoch": 2.9,
      "grad_norm": 6.654289245605469,
      "learning_rate": 2.1e-05,
      "loss": 0.604,
      "step": 2900
    },
    {
      "epoch": 2.91,
      "grad_norm": 2.984414577484131,
      "learning_rate": 2.09e-05,
      "loss": 0.3169,
      "step": 2910
    },
    {
      "epoch": 2.92,
      "grad_norm": 10.009943008422852,
      "learning_rate": 2.08e-05,
      "loss": 0.2989,
      "step": 2920
    },
    {
      "epoch": 2.93,
      "grad_norm": 5.655477523803711,
      "learning_rate": 2.07e-05,
      "loss": 0.3196,
      "step": 2930
    },
    {
      "epoch": 2.94,
      "grad_norm": 24.138044357299805,
      "learning_rate": 2.06e-05,
      "loss": 0.4875,
      "step": 2940
    },
    {
      "epoch": 2.95,
      "grad_norm": 9.849169731140137,
      "learning_rate": 2.05e-05,
      "loss": 0.4402,
      "step": 2950
    },
    {
      "epoch": 2.96,
      "grad_norm": 3.6115739345550537,
      "learning_rate": 2.04e-05,
      "loss": 0.3861,
      "step": 2960
    },
    {
      "epoch": 2.97,
      "grad_norm": 5.626909255981445,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 0.4135,
      "step": 2970
    },
    {
      "epoch": 2.98,
      "grad_norm": 6.157430648803711,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.3391,
      "step": 2980
    },
    {
      "epoch": 2.99,
      "grad_norm": 2.563890218734741,
      "learning_rate": 2.01e-05,
      "loss": 0.419,
      "step": 2990
    },
    {
      "epoch": 3.0,
      "grad_norm": 5.230432510375977,
      "learning_rate": 2e-05,
      "loss": 0.3158,
      "step": 3000
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8875,
      "eval_loss": 0.3743281662464142,
      "eval_runtime": 4.0798,
      "eval_samples_per_second": 490.215,
      "eval_steps_per_second": 7.843,
      "step": 3000
    },
    {
      "epoch": 3.01,
      "grad_norm": 2.630244493484497,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.3629,
      "step": 3010
    },
    {
      "epoch": 3.02,
      "grad_norm": 17.090538024902344,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.4212,
      "step": 3020
    },
    {
      "epoch": 3.03,
      "grad_norm": 10.847615242004395,
      "learning_rate": 1.97e-05,
      "loss": 0.5627,
      "step": 3030
    },
    {
      "epoch": 3.04,
      "grad_norm": 5.266941547393799,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.3043,
      "step": 3040
    },
    {
      "epoch": 3.05,
      "grad_norm": 6.032543182373047,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.3697,
      "step": 3050
    },
    {
      "epoch": 3.06,
      "grad_norm": 9.082342147827148,
      "learning_rate": 1.94e-05,
      "loss": 0.3064,
      "step": 3060
    },
    {
      "epoch": 3.07,
      "grad_norm": 11.36451530456543,
      "learning_rate": 1.93e-05,
      "loss": 0.4408,
      "step": 3070
    },
    {
      "epoch": 3.08,
      "grad_norm": 9.487717628479004,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.3094,
      "step": 3080
    },
    {
      "epoch": 3.09,
      "grad_norm": 9.137226104736328,
      "learning_rate": 1.91e-05,
      "loss": 0.4173,
      "step": 3090
    },
    {
      "epoch": 3.1,
      "grad_norm": 15.341733932495117,
      "learning_rate": 1.9e-05,
      "loss": 0.3112,
      "step": 3100
    },
    {
      "epoch": 3.11,
      "grad_norm": 6.393512725830078,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.3157,
      "step": 3110
    },
    {
      "epoch": 3.12,
      "grad_norm": 19.141929626464844,
      "learning_rate": 1.88e-05,
      "loss": 0.3289,
      "step": 3120
    },
    {
      "epoch": 3.13,
      "grad_norm": 10.44216537475586,
      "learning_rate": 1.87e-05,
      "loss": 0.3845,
      "step": 3130
    },
    {
      "epoch": 3.14,
      "grad_norm": 9.47485065460205,
      "learning_rate": 1.86e-05,
      "loss": 0.3015,
      "step": 3140
    },
    {
      "epoch": 3.15,
      "grad_norm": 12.000354766845703,
      "learning_rate": 1.85e-05,
      "loss": 0.3746,
      "step": 3150
    },
    {
      "epoch": 3.16,
      "grad_norm": 8.800923347473145,
      "learning_rate": 1.84e-05,
      "loss": 0.3371,
      "step": 3160
    },
    {
      "epoch": 3.17,
      "grad_norm": 11.449569702148438,
      "learning_rate": 1.83e-05,
      "loss": 0.417,
      "step": 3170
    },
    {
      "epoch": 3.18,
      "grad_norm": 5.219265460968018,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.3549,
      "step": 3180
    },
    {
      "epoch": 3.19,
      "grad_norm": 9.198051452636719,
      "learning_rate": 1.81e-05,
      "loss": 0.3668,
      "step": 3190
    },
    {
      "epoch": 3.2,
      "grad_norm": 15.118261337280273,
      "learning_rate": 1.8e-05,
      "loss": 0.3436,
      "step": 3200
    },
    {
      "epoch": 3.21,
      "grad_norm": 10.610028266906738,
      "learning_rate": 1.79e-05,
      "loss": 0.4041,
      "step": 3210
    },
    {
      "epoch": 3.22,
      "grad_norm": 8.589820861816406,
      "learning_rate": 1.78e-05,
      "loss": 0.3328,
      "step": 3220
    },
    {
      "epoch": 3.23,
      "grad_norm": 2.91501784324646,
      "learning_rate": 1.77e-05,
      "loss": 0.3535,
      "step": 3230
    },
    {
      "epoch": 3.24,
      "grad_norm": 4.158052921295166,
      "learning_rate": 1.76e-05,
      "loss": 0.2601,
      "step": 3240
    },
    {
      "epoch": 3.25,
      "grad_norm": 2.1055569648742676,
      "learning_rate": 1.75e-05,
      "loss": 0.3796,
      "step": 3250
    },
    {
      "epoch": 3.26,
      "grad_norm": 14.321949005126953,
      "learning_rate": 1.74e-05,
      "loss": 0.3001,
      "step": 3260
    },
    {
      "epoch": 3.27,
      "grad_norm": 11.561120986938477,
      "learning_rate": 1.73e-05,
      "loss": 0.2712,
      "step": 3270
    },
    {
      "epoch": 3.28,
      "grad_norm": 1.3510621786117554,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.255,
      "step": 3280
    },
    {
      "epoch": 3.29,
      "grad_norm": 14.573076248168945,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.3618,
      "step": 3290
    },
    {
      "epoch": 3.3,
      "grad_norm": 12.468243598937988,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.3613,
      "step": 3300
    },
    {
      "epoch": 3.31,
      "grad_norm": 11.6408052444458,
      "learning_rate": 1.69e-05,
      "loss": 0.3374,
      "step": 3310
    },
    {
      "epoch": 3.32,
      "grad_norm": 11.427055358886719,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.2849,
      "step": 3320
    },
    {
      "epoch": 3.33,
      "grad_norm": 8.257050514221191,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.3235,
      "step": 3330
    },
    {
      "epoch": 3.34,
      "grad_norm": 6.158411026000977,
      "learning_rate": 1.66e-05,
      "loss": 0.2892,
      "step": 3340
    },
    {
      "epoch": 3.35,
      "grad_norm": 10.497249603271484,
      "learning_rate": 1.65e-05,
      "loss": 0.2789,
      "step": 3350
    },
    {
      "epoch": 3.36,
      "grad_norm": 14.694751739501953,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.4891,
      "step": 3360
    },
    {
      "epoch": 3.37,
      "grad_norm": 8.199350357055664,
      "learning_rate": 1.63e-05,
      "loss": 0.3798,
      "step": 3370
    },
    {
      "epoch": 3.38,
      "grad_norm": 18.71870231628418,
      "learning_rate": 1.62e-05,
      "loss": 0.4193,
      "step": 3380
    },
    {
      "epoch": 3.39,
      "grad_norm": 16.02541160583496,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.3775,
      "step": 3390
    },
    {
      "epoch": 3.4,
      "grad_norm": 6.3035664558410645,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.3852,
      "step": 3400
    },
    {
      "epoch": 3.41,
      "grad_norm": 10.008957862854004,
      "learning_rate": 1.59e-05,
      "loss": 0.3204,
      "step": 3410
    },
    {
      "epoch": 3.42,
      "grad_norm": 17.506790161132812,
      "learning_rate": 1.58e-05,
      "loss": 0.4741,
      "step": 3420
    },
    {
      "epoch": 3.43,
      "grad_norm": 16.410598754882812,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.3143,
      "step": 3430
    },
    {
      "epoch": 3.44,
      "grad_norm": 8.146533966064453,
      "learning_rate": 1.56e-05,
      "loss": 0.2349,
      "step": 3440
    },
    {
      "epoch": 3.45,
      "grad_norm": 15.567546844482422,
      "learning_rate": 1.55e-05,
      "loss": 0.2816,
      "step": 3450
    },
    {
      "epoch": 3.46,
      "grad_norm": 3.056216239929199,
      "learning_rate": 1.54e-05,
      "loss": 0.494,
      "step": 3460
    },
    {
      "epoch": 3.47,
      "grad_norm": 7.13253116607666,
      "learning_rate": 1.53e-05,
      "loss": 0.3707,
      "step": 3470
    },
    {
      "epoch": 3.48,
      "grad_norm": 2.987684965133667,
      "learning_rate": 1.52e-05,
      "loss": 0.3701,
      "step": 3480
    },
    {
      "epoch": 3.49,
      "grad_norm": 8.339008331298828,
      "learning_rate": 1.51e-05,
      "loss": 0.322,
      "step": 3490
    },
    {
      "epoch": 3.5,
      "grad_norm": 5.290161609649658,
      "learning_rate": 1.5e-05,
      "loss": 0.3296,
      "step": 3500
    },
    {
      "epoch": 3.51,
      "grad_norm": 15.471970558166504,
      "learning_rate": 1.49e-05,
      "loss": 0.3735,
      "step": 3510
    },
    {
      "epoch": 3.52,
      "grad_norm": 9.245373725891113,
      "learning_rate": 1.48e-05,
      "loss": 0.3393,
      "step": 3520
    },
    {
      "epoch": 3.53,
      "grad_norm": 1.0814673900604248,
      "learning_rate": 1.47e-05,
      "loss": 0.3303,
      "step": 3530
    },
    {
      "epoch": 3.54,
      "grad_norm": 5.517128944396973,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.2799,
      "step": 3540
    },
    {
      "epoch": 3.55,
      "grad_norm": 8.137192726135254,
      "learning_rate": 1.45e-05,
      "loss": 0.3241,
      "step": 3550
    },
    {
      "epoch": 3.56,
      "grad_norm": 2.7494542598724365,
      "learning_rate": 1.44e-05,
      "loss": 0.2427,
      "step": 3560
    },
    {
      "epoch": 3.57,
      "grad_norm": 8.37611198425293,
      "learning_rate": 1.43e-05,
      "loss": 0.4388,
      "step": 3570
    },
    {
      "epoch": 3.58,
      "grad_norm": 4.2512736320495605,
      "learning_rate": 1.42e-05,
      "loss": 0.2784,
      "step": 3580
    },
    {
      "epoch": 3.59,
      "grad_norm": 1.3651193380355835,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 0.4455,
      "step": 3590
    },
    {
      "epoch": 3.6,
      "grad_norm": 6.645131587982178,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.2773,
      "step": 3600
    },
    {
      "epoch": 3.61,
      "grad_norm": 4.294663906097412,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.3512,
      "step": 3610
    },
    {
      "epoch": 3.62,
      "grad_norm": 3.5145103931427,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.3522,
      "step": 3620
    },
    {
      "epoch": 3.63,
      "grad_norm": 10.101155281066895,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.4135,
      "step": 3630
    },
    {
      "epoch": 3.64,
      "grad_norm": 10.063026428222656,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.3388,
      "step": 3640
    },
    {
      "epoch": 3.65,
      "grad_norm": 5.634901523590088,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.4554,
      "step": 3650
    },
    {
      "epoch": 3.66,
      "grad_norm": 14.176279067993164,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.2988,
      "step": 3660
    },
    {
      "epoch": 3.67,
      "grad_norm": 11.65241813659668,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 0.4451,
      "step": 3670
    },
    {
      "epoch": 3.68,
      "grad_norm": 4.826048851013184,
      "learning_rate": 1.32e-05,
      "loss": 0.276,
      "step": 3680
    },
    {
      "epoch": 3.69,
      "grad_norm": 5.08603572845459,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.4598,
      "step": 3690
    },
    {
      "epoch": 3.7,
      "grad_norm": 17.30914306640625,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.3678,
      "step": 3700
    },
    {
      "epoch": 3.71,
      "grad_norm": 9.316654205322266,
      "learning_rate": 1.29e-05,
      "loss": 0.3194,
      "step": 3710
    },
    {
      "epoch": 3.72,
      "grad_norm": 14.739590644836426,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.3446,
      "step": 3720
    },
    {
      "epoch": 3.73,
      "grad_norm": 12.321136474609375,
      "learning_rate": 1.27e-05,
      "loss": 0.301,
      "step": 3730
    },
    {
      "epoch": 3.74,
      "grad_norm": 11.85169792175293,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.326,
      "step": 3740
    },
    {
      "epoch": 3.75,
      "grad_norm": 20.100069046020508,
      "learning_rate": 1.25e-05,
      "loss": 0.2467,
      "step": 3750
    },
    {
      "epoch": 3.76,
      "grad_norm": 11.128853797912598,
      "learning_rate": 1.24e-05,
      "loss": 0.4595,
      "step": 3760
    },
    {
      "epoch": 3.77,
      "grad_norm": 17.29488754272461,
      "learning_rate": 1.23e-05,
      "loss": 0.3622,
      "step": 3770
    },
    {
      "epoch": 3.78,
      "grad_norm": 10.692028045654297,
      "learning_rate": 1.22e-05,
      "loss": 0.3676,
      "step": 3780
    },
    {
      "epoch": 3.79,
      "grad_norm": 5.569849014282227,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 0.3708,
      "step": 3790
    },
    {
      "epoch": 3.8,
      "grad_norm": 7.848878860473633,
      "learning_rate": 1.2e-05,
      "loss": 0.3363,
      "step": 3800
    },
    {
      "epoch": 3.81,
      "grad_norm": 10.40623950958252,
      "learning_rate": 1.19e-05,
      "loss": 0.327,
      "step": 3810
    },
    {
      "epoch": 3.82,
      "grad_norm": 10.206122398376465,
      "learning_rate": 1.18e-05,
      "loss": 0.2596,
      "step": 3820
    },
    {
      "epoch": 3.83,
      "grad_norm": 7.055304050445557,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.3282,
      "step": 3830
    },
    {
      "epoch": 3.84,
      "grad_norm": 33.559791564941406,
      "learning_rate": 1.16e-05,
      "loss": 0.3151,
      "step": 3840
    },
    {
      "epoch": 3.85,
      "grad_norm": 5.741425037384033,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.2883,
      "step": 3850
    },
    {
      "epoch": 3.86,
      "grad_norm": 7.77947998046875,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.4461,
      "step": 3860
    },
    {
      "epoch": 3.87,
      "grad_norm": 4.660419464111328,
      "learning_rate": 1.13e-05,
      "loss": 0.3246,
      "step": 3870
    },
    {
      "epoch": 3.88,
      "grad_norm": 10.213163375854492,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.2854,
      "step": 3880
    },
    {
      "epoch": 3.89,
      "grad_norm": 6.963132858276367,
      "learning_rate": 1.11e-05,
      "loss": 0.3292,
      "step": 3890
    },
    {
      "epoch": 3.9,
      "grad_norm": 12.142866134643555,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.388,
      "step": 3900
    },
    {
      "epoch": 3.91,
      "grad_norm": 6.295760631561279,
      "learning_rate": 1.09e-05,
      "loss": 0.3465,
      "step": 3910
    },
    {
      "epoch": 3.92,
      "grad_norm": 18.0087890625,
      "learning_rate": 1.08e-05,
      "loss": 0.3109,
      "step": 3920
    },
    {
      "epoch": 3.93,
      "grad_norm": 17.790433883666992,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.3545,
      "step": 3930
    },
    {
      "epoch": 3.94,
      "grad_norm": 16.11830711364746,
      "learning_rate": 1.06e-05,
      "loss": 0.4277,
      "step": 3940
    },
    {
      "epoch": 3.95,
      "grad_norm": 15.302557945251465,
      "learning_rate": 1.05e-05,
      "loss": 0.338,
      "step": 3950
    },
    {
      "epoch": 3.96,
      "grad_norm": 9.020286560058594,
      "learning_rate": 1.04e-05,
      "loss": 0.328,
      "step": 3960
    },
    {
      "epoch": 3.97,
      "grad_norm": 11.851091384887695,
      "learning_rate": 1.03e-05,
      "loss": 0.4174,
      "step": 3970
    },
    {
      "epoch": 3.98,
      "grad_norm": 6.347015380859375,
      "learning_rate": 1.02e-05,
      "loss": 0.3972,
      "step": 3980
    },
    {
      "epoch": 3.99,
      "grad_norm": 13.740161895751953,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 0.3149,
      "step": 3990
    },
    {
      "epoch": 4.0,
      "grad_norm": 10.324834823608398,
      "learning_rate": 1e-05,
      "loss": 0.3319,
      "step": 4000
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.8995,
      "eval_loss": 0.34414759278297424,
      "eval_runtime": 4.0422,
      "eval_samples_per_second": 494.779,
      "eval_steps_per_second": 7.916,
      "step": 4000
    }
  ],
  "logging_steps": 10,
  "max_steps": 5000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "total_flos": 20353155072000.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
